---------------------------ETL Postgres---------------
Sửa trong file mage:
dev:
    POSTGRES_CONNECT_TIMEOUT: 10
    POSTGRES_DBNAME: "{{ env_var('POSTGRES_DBNAME')}}"
    POSTGRES_SCHEMA: "{{ env_var('POSTGRES_SCHEMA')}}"
    POSTGRES_USER: "{{ env_var('POSTGRES_USER')}}"
    POSTGRES_PASSWORD: "{{ env_var('POSTGRES_PASSWORD')}}"
    POSTGRES_HOST: "{{ env_var('POSTGRES_HOST')}}"
    POSTGRES_PORT: "{{ env_var('POSTGRES_PORT')}}"
Tiếp theo vào file -> new pipeline -> standard(batch) -> edit -> pipeline setting -> rename -> save 
pipeline load_api_data -> transform_data -> taxi_data_to_postgres -> load_taxi_data
tiếp theo chọn dataloader -> python -> generic 
    import io
    import pandas as pd
    import requests
    if 'data_loader' not in globals():
        from mage_ai.data_preparation.decorators import data_loader
    if 'test' not in globals():
        from mage_ai.data_preparation.decorators import test


    @data_loader
    def load_data_from_api(*args, **kwargs):
        """
        Template for loading data from API
        """
        url = 'https://github.com/DataTalksClub/nyc-tlc-data/releases/download/yellow/yellow_tripdata_2021-01.csv.gz'

        taxi_dtypes = {
            'VendorID': pd.Int64Dtype(),
            'passenger_count': pd.Int64Dtype(),
            'trip_distance': float,
            'RatecodeID': pd.Int64Dtype(),
            'store_and_fwd_flag': str,
            'PULocationID': pd.Int64Dtype(),
            'DOLocationID': pd.Int64Dtype(),
            'payment_type': pd.Int64Dtype(),
            'fare_amount': float,
            'extra': float,
            'mta_tax': float,
            'tip_amount': float,
            'tolls_amount': float,
            'improvement_surcharge': float,
            'total_amount': float,
            'congestion_surcharge': float 
        }
        
        parse_dates = ['tpep_pickup_datetime','tpep_dropoff_datetime']

        return pd.read_csv(url,sep=",",compression="gzip",dtype=taxi_dtypes, parse_dates=parse_dates)

    @test
    def test_output(output, *args) -> None:
        """
        Template code for testing the output of the block.
        """
        assert output is not None, 'The output is undefined'



transform_data
      if 'transformer' not in globals():
          from mage_ai.data_preparation.decorators import transformer
      if 'test' not in globals():
          from mage_ai.data_preparation.decorators import test


      @transformer
      def transform(data, *args, **kwargs):
        print("Rows with zero passengers:",data['passenger_count'].isin([0]).sum())

        return data[data['passenger_count']>0]

      @test
      def test_output(output, *args):
          assert output['passenger_count'].isin([0]).sum() == 0, 'There are rides with zero passenger'

taxi_data_to_postgres
      from mage_ai.settings.repo import get_repo_path
      from mage_ai.io.config import ConfigFileLoader
      from mage_ai.io.postgres import Postgres
      from pandas import DataFrame
      from os import path

      if 'data_exporter' not in globals():
          from mage_ai.data_preparation.decorators import data_exporter


      @data_exporter
      def export_data_to_postgres(df: DataFrame, **kwargs) -> None:
          """
          Template for exporting data to a PostgreSQL database.
          Specify your configuration settings in 'io_config.yaml'.

          Docs: https://docs.mage.ai/design/data-loading#postgresql
          """
          schema_name = 'ny_taxi'  # Specify the name of the schema to export data to
          table_name = 'yellow_cab_data'  # Specify the name of the table to export data to
          config_path = path.join(get_repo_path(), 'io_config.yaml')
          config_profile = 'dev'

          with Postgres.with_config(ConfigFileLoader(config_path, config_profile)) as loader:
              loader.export(
                  df,
                  schema_name,
                  table_name,
                  index=False,  # Specifies whether to include index in exported table
                  if_exists='replace',  # Specify resolution policy if table name already exists
              )

load_taxi_data


------------------------------------ETL GCS ------------------------------------------------------
Bước 1: Tạo google cloud storeage -> bucket
Bươc 2: Tạo 1 service account -> tạo key
Bước 3: Sửa file trong mage phần GOOGLE

load_api_data -> clean_taxi_data(taxi_data_to_postgres) -> data_to_gcs_parquet, taxi_to_gcs_partitioned_parquet

data_to_gcs_parquet
    from mage_ai.settings.repo import get_repo_path
    from mage_ai.io.config import ConfigFileLoader
    from mage_ai.io.google_cloud_storage import GoogleCloudStorage
    from pandas import DataFrame
    from os import path

    if 'data_exporter' not in globals():
        from mage_ai.data_preparation.decorators import data_exporter


    @data_exporter
    def export_data_to_google_cloud_storage(df: DataFrame, **kwargs) -> None:
        """
        Template for exporting data to a Google Cloud Storage bucket.
        Specify your configuration settings in 'io_config.yaml'.

        Docs: https://docs.mage.ai/design/data-loading#googlecloudstorage
        """
        config_path = path.join(get_repo_path(), 'io_config.yaml')
        config_profile = 'default'

        bucket_name = 'mage-zoomcamp-nghia-1'
        object_key = 'nyc_taxi_data.parquet'

        GoogleCloudStorage.with_config(ConfigFileLoader(config_path, config_profile)).export(
            df,
            bucket_name,
            object_key,
        )


taxi_to_gcs_partitioned_parquet
    import pyarrow as pa
    import pyarrow.parquet as pq
    import os



    if 'data_exporter' not in globals():
        from mage_ai.data_preparation.decorators import data_exporter


    os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = "diesel-octane-416402-c18e4ee5922e.json"
    bucket_name = 'mage-zoomcamp-nghia-1'
    project_id = 'diesel-octane-416402'

    table_name = "nyc_taxi_data"

    root_path = f'{bucket_name}/{table_name}'

    @data_exporter
    def export_data(data, *args, **kwargs):
        data['tpep_pickup_date'] = data['tpep_pickup_datetime'].dt.date

        table = pa.Table.from_pandas(data)

        gcs = pa.fs.GcsFileSystem()

        pq.write_to_dataset(
            table,
            root_path=root_path,
            partition_cols=['tpep_pickup_date'],
            filesystem=gcs
        )
        """
        Exports data to some source.

        Args:
            data: The output from the upstream parent block
            args: The output from any additional upstream blocks (if applicable)

        Output (optional):
            Optionally return any object and it'll be logged and
            displayed when inspecting the block run.
        """
        # Specify your data exporting logic here



-------------------------ETL: GCS to BigQuery--------------
load_taxi_gcs -> transform_state_data ->sql


load_taxi_gcs

    from mage_ai.settings.repo import get_repo_path
    from mage_ai.io.config import ConfigFileLoader
    from mage_ai.io.google_cloud_storage import GoogleCloudStorage
    from os import path
    if 'data_loader' not in globals():
        from mage_ai.data_preparation.decorators import data_loader
    if 'test' not in globals():
        from mage_ai.data_preparation.decorators import test


    @data_loader
    def load_from_google_cloud_storage(*args, **kwargs):
        """
        Template for loading data from a Google Cloud Storage bucket.
        Specify your configuration settings in 'io_config.yaml'.

        Docs: https://docs.mage.ai/design/data-loading#googlecloudstorage
        """
        config_path = path.join(get_repo_path(), 'io_config.yaml')
        config_profile = 'default'

        bucket_name = 'mage-zoomcamp-nghia-1'
        object_key = 'nyc_taxi_data.parquet'

        return GoogleCloudStorage.with_config(ConfigFileLoader(config_path, config_profile)).load(
            bucket_name,
            object_key,
        )

transform_state_data
    if 'transformer' not in globals():
        from mage_ai.data_preparation.decorators import transformer
    if 'test' not in globals():
        from mage_ai.data_preparation.decorators import test


    @transformer
    def transform(data, *args, **kwargs):
        data.columns = (data.columns
                        .str.replace(' ','_')
                        .str.lower()      
        )
        return data


------------------------Parameterized Execution -------------------------
Bước 1: clone lại pipeline wise-water
Bước 2: xóa hết data exporter tạo 1 cái mới export_taxi_to_gcp_parameter

    from mage_ai.settings.repo import get_repo_path
    from mage_ai.io.config import ConfigFileLoader
    from mage_ai.io.google_cloud_storage import GoogleCloudStorage
    from pandas import DataFrame
    from os import path

    if 'data_exporter' not in globals():
        from mage_ai.data_preparation.decorators import data_exporter


    @data_exporter
    def export_data_to_google_cloud_storage(df: DataFrame, **kwargs) -> None:
        """
        Template for exporting data to a Google Cloud Storage bucket.
        Specify your configuration settings in 'io_config.yaml'.

        Docs: https://docs.mage.ai/design/data-loading#googlecloudstorage


        """
        now = kwargs.get('execution_date')
        now_fpath = now.strftime("%Y/%m/%d")
        
        # print(now)
        # print(now.date())

        # #format laij ngày tháng

        # print(now.strftime("%Y/%m/%d"))

        config_path = path.join(get_repo_path(), 'io_config.yaml')
        config_profile = 'default'

        bucket_name = 'mage-zoomcamp-nghia-1'
        object_key = f'{now_fpath}/nyc_taxi_data.parquet'

        print(object_key)

        GoogleCloudStorage.with_config(ConfigFileLoader(config_path, config_profile)).export(
            df,
            bucket_name,
            object_key,
        )


----------------   Backfills ----------------------------------
Có thể hiểu là 1 dag đã chạy trong 1 tháng trước, tháng này có thêm 1 task mới nhưng cần thực hiện lại đối với các dag đã chạy ở trước thì dùng Backfills

-------------------deloy mage to gcp --------------------------

Bước 1 : quyền google cloud : tạo tài khoản acount cấp quyền và tạo keys
Bước 2 : 




